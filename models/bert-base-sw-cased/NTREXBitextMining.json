{
  "dataset_revision": "ed9a4403ed4adbfaf4aab56d5b2709e9f6c3ba33",
  "evaluation_time": 64.38607788085938,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "accuracy": 0.005007511266900351,
        "f1": 0.003966243482871366,
        "hf_subset": "amh_Ethi-swa_Latn",
        "languages": [
          "amh-Ethi",
          "swa-Latn"
        ],
        "main_score": 0.003966243482871366,
        "precision": 0.0038281531193787594,
        "recall": 0.005007511266900351
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0014831567704100804,
        "hf_subset": "arb_Arab-swa_Latn",
        "languages": [
          "arb-Arab",
          "swa-Latn"
        ],
        "main_score": 0.0014831567704100804,
        "precision": 0.0013307096127755688,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0021488505017122042,
        "hf_subset": "ben_Beng-swa_Latn",
        "languages": [
          "ben-Beng",
          "swa-Latn"
        ],
        "main_score": 0.0021488505017122042,
        "precision": 0.002087854003227063,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.1397095643465198,
        "f1": 0.10719348138989272,
        "hf_subset": "deu_Latn-swa_Latn",
        "languages": [
          "deu-Latn",
          "swa-Latn"
        ],
        "main_score": 0.10719348138989272,
        "precision": 0.09909297486448873,
        "recall": 0.1397095643465198
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.004712225868090372,
        "hf_subset": "ell_Grek-swa_Latn",
        "languages": [
          "ell-Grek",
          "swa-Latn"
        ],
        "main_score": 0.004712225868090372,
        "precision": 0.004174151647040922,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.20681021532298446,
        "f1": 0.1680315757419174,
        "hf_subset": "eng_Latn-swa_Latn",
        "languages": [
          "eng-Latn",
          "swa-Latn"
        ],
        "main_score": 0.1680315757419174,
        "precision": 0.15778630506646604,
        "recall": 0.20681021532298446
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.004977371590449101,
        "hf_subset": "fas_Arab-swa_Latn",
        "languages": [
          "fas-Arab",
          "swa-Latn"
        ],
        "main_score": 0.004977371590449101,
        "precision": 0.004687210996675193,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.12118177265898848,
        "f1": 0.09656191042812777,
        "hf_subset": "fin_Latn-swa_Latn",
        "languages": [
          "fin-Latn",
          "swa-Latn"
        ],
        "main_score": 0.09656191042812777,
        "precision": 0.09066212350531039,
        "recall": 0.12118177265898848
      },
      {
        "accuracy": 0.10966449674511768,
        "f1": 0.08517351706677109,
        "hf_subset": "fra_Latn-swa_Latn",
        "languages": [
          "fra-Latn",
          "swa-Latn"
        ],
        "main_score": 0.08517351706677109,
        "precision": 0.07995850753524825,
        "recall": 0.10966449674511768
      },
      {
        "accuracy": 0.05608412618928393,
        "f1": 0.0484694644624346,
        "hf_subset": "hau_Latn-swa_Latn",
        "languages": [
          "hau-Latn",
          "swa-Latn"
        ],
        "main_score": 0.0484694644624346,
        "precision": 0.04690714846080587,
        "recall": 0.05608412618928393
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.003079671293472106,
        "hf_subset": "heb_Hebr-swa_Latn",
        "languages": [
          "heb-Hebr",
          "swa-Latn"
        ],
        "main_score": 0.003079671293472106,
        "precision": 0.0028339552468460596,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.004506760140210316,
        "f1": 0.0033393256059113248,
        "hf_subset": "hin_Deva-swa_Latn",
        "languages": [
          "hin-Deva",
          "swa-Latn"
        ],
        "main_score": 0.0033393256059113248,
        "precision": 0.0031051498507603924,
        "recall": 0.004506760140210316
      },
      {
        "accuracy": 0.06359539308963445,
        "f1": 0.045710841810852214,
        "hf_subset": "hun_Latn-swa_Latn",
        "languages": [
          "hun-Latn",
          "swa-Latn"
        ],
        "main_score": 0.045710841810852214,
        "precision": 0.041940588159455956,
        "recall": 0.06359539308963445
      },
      {
        "accuracy": 0.04356534802203305,
        "f1": 0.03904783504594804,
        "hf_subset": "ibo_Latn-swa_Latn",
        "languages": [
          "ibo-Latn",
          "swa-Latn"
        ],
        "main_score": 0.03904783504594804,
        "precision": 0.03807986297577357,
        "recall": 0.04356534802203305
      },
      {
        "accuracy": 0.20781171757636455,
        "f1": 0.16989115062187368,
        "hf_subset": "ind_Latn-swa_Latn",
        "languages": [
          "ind-Latn",
          "swa-Latn"
        ],
        "main_score": 0.16989115062187368,
        "precision": 0.16070026765601464,
        "recall": 0.20781171757636455
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.0025061864641622625,
        "hf_subset": "jpn_Jpan-swa_Latn",
        "languages": [
          "jpn-Jpan",
          "swa-Latn"
        ],
        "main_score": 0.0025061864641622625,
        "precision": 0.002504974006021197,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.00400600901352028,
        "f1": 0.0026313222588129066,
        "hf_subset": "kor_Hang-swa_Latn",
        "languages": [
          "kor-Hang",
          "swa-Latn"
        ],
        "main_score": 0.0026313222588129066,
        "precision": 0.0025764824635868934,
        "recall": 0.00400600901352028
      },
      {
        "accuracy": 0.08112168252378568,
        "f1": 0.05530902442234492,
        "hf_subset": "lit_Latn-swa_Latn",
        "languages": [
          "lit-Latn",
          "swa-Latn"
        ],
        "main_score": 0.05530902442234492,
        "precision": 0.05042746454501523,
        "recall": 0.08112168252378568
      },
      {
        "accuracy": 0.15673510265398097,
        "f1": 0.1264538387311301,
        "hf_subset": "nld_Latn-swa_Latn",
        "languages": [
          "nld-Latn",
          "swa-Latn"
        ],
        "main_score": 0.1264538387311301,
        "precision": 0.11904227880043738,
        "recall": 0.15673510265398097
      },
      {
        "accuracy": 0.05658487731597396,
        "f1": 0.047518939832628636,
        "hf_subset": "nso_Latn-swa_Latn",
        "languages": [
          "nso-Latn",
          "swa-Latn"
        ],
        "main_score": 0.047518939832628636,
        "precision": 0.045595593484571914,
        "recall": 0.05658487731597396
      },
      {
        "accuracy": 0.013019529293940912,
        "f1": 0.009988400214941741,
        "hf_subset": "orm_Ethi-swa_Latn",
        "languages": [
          "orm-Ethi",
          "swa-Latn"
        ],
        "main_score": 0.009988400214941741,
        "precision": 0.00953060077583485,
        "recall": 0.013019529293940912
      },
      {
        "accuracy": 0.11417125688532799,
        "f1": 0.08420973531294569,
        "hf_subset": "pol_Latn-swa_Latn",
        "languages": [
          "pol-Latn",
          "swa-Latn"
        ],
        "main_score": 0.08420973531294569,
        "precision": 0.0774070024735964,
        "recall": 0.11417125688532799
      },
      {
        "accuracy": 0.14822233350025038,
        "f1": 0.11950119720471464,
        "hf_subset": "por_Latn-swa_Latn",
        "languages": [
          "por-Latn",
          "swa-Latn"
        ],
        "main_score": 0.11950119720471464,
        "precision": 0.11287745542604147,
        "recall": 0.14822233350025038
      },
      {
        "accuracy": 0.006509764646970456,
        "f1": 0.0052996160908028705,
        "hf_subset": "rus_Cyrl-swa_Latn",
        "languages": [
          "rus-Cyrl",
          "swa-Latn"
        ],
        "main_score": 0.0052996160908028705,
        "precision": 0.005012280325249779,
        "recall": 0.006509764646970456
      },
      {
        "accuracy": 0.02904356534802203,
        "f1": 0.022463308117811377,
        "hf_subset": "som_Latn-swa_Latn",
        "languages": [
          "som-Latn",
          "swa-Latn"
        ],
        "main_score": 0.022463308117811377,
        "precision": 0.021142057429654453,
        "recall": 0.02904356534802203
      },
      {
        "accuracy": 0.1557336004006009,
        "f1": 0.12052308831245195,
        "hf_subset": "spa_Latn-swa_Latn",
        "languages": [
          "spa-Latn",
          "swa-Latn"
        ],
        "main_score": 0.12052308831245195,
        "precision": 0.11178980774589181,
        "recall": 0.1557336004006009
      },
      {
        "accuracy": 0.03805708562844266,
        "f1": 0.03119825437609147,
        "hf_subset": "ssw_Latn-swa_Latn",
        "languages": [
          "ssw-Latn",
          "swa-Latn"
        ],
        "main_score": 0.03119825437609147,
        "precision": 0.029816367093967577,
        "recall": 0.03805708562844266
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.004297678544050765,
        "hf_subset": "swa_Latn-amh_Ethi",
        "languages": [
          "swa-Latn",
          "amh-Ethi"
        ],
        "main_score": 0.004297678544050765,
        "precision": 0.0030718151454825655,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.008512769153730596,
        "f1": 0.0039609565704351,
        "hf_subset": "swa_Latn-arb_Arab",
        "languages": [
          "swa-Latn",
          "arb-Arab"
        ],
        "main_score": 0.0039609565704351,
        "precision": 0.0033574049117137696,
        "recall": 0.008512769153730596
      },
      {
        "accuracy": 0.011016524787180772,
        "f1": 0.0033856566202239377,
        "hf_subset": "swa_Latn-ben_Beng",
        "languages": [
          "swa-Latn",
          "ben-Beng"
        ],
        "main_score": 0.0033856566202239377,
        "precision": 0.002717844848219198,
        "recall": 0.011016524787180772
      },
      {
        "accuracy": 0.1402103154732098,
        "f1": 0.10466983682892098,
        "hf_subset": "swa_Latn-deu_Latn",
        "languages": [
          "swa-Latn",
          "deu-Latn"
        ],
        "main_score": 0.10466983682892098,
        "precision": 0.09633898771946114,
        "recall": 0.1402103154732098
      },
      {
        "accuracy": 0.015523284927391086,
        "f1": 0.008558637098547602,
        "hf_subset": "swa_Latn-ell_Grek",
        "languages": [
          "swa-Latn",
          "ell-Grek"
        ],
        "main_score": 0.008558637098547602,
        "precision": 0.0073732742899329734,
        "recall": 0.015523284927391086
      },
      {
        "accuracy": 0.14471707561342012,
        "f1": 0.11844287043416342,
        "hf_subset": "swa_Latn-eng_Latn",
        "languages": [
          "swa-Latn",
          "eng-Latn"
        ],
        "main_score": 0.11844287043416342,
        "precision": 0.11267242344365119,
        "recall": 0.14471707561342012
      },
      {
        "accuracy": 0.014021031547320982,
        "f1": 0.007102275182863647,
        "hf_subset": "swa_Latn-fas_Arab",
        "languages": [
          "swa-Latn",
          "fas-Arab"
        ],
        "main_score": 0.007102275182863647,
        "precision": 0.006113246232403734,
        "recall": 0.014021031547320982
      },
      {
        "accuracy": 0.13820731096644967,
        "f1": 0.10091170670392619,
        "hf_subset": "swa_Latn-fin_Latn",
        "languages": [
          "swa-Latn",
          "fin-Latn"
        ],
        "main_score": 0.10091170670392619,
        "precision": 0.09249928986181476,
        "recall": 0.13820731096644967
      },
      {
        "accuracy": 0.11116675012518779,
        "f1": 0.08104571666212416,
        "hf_subset": "swa_Latn-fra_Latn",
        "languages": [
          "swa-Latn",
          "fra-Latn"
        ],
        "main_score": 0.08104571666212416,
        "precision": 0.0756429166886957,
        "recall": 0.11116675012518779
      },
      {
        "accuracy": 0.0500751126690035,
        "f1": 0.03325251180820832,
        "hf_subset": "swa_Latn-hau_Latn",
        "languages": [
          "swa-Latn",
          "hau-Latn"
        ],
        "main_score": 0.03325251180820832,
        "precision": 0.03065127794550274,
        "recall": 0.0500751126690035
      },
      {
        "accuracy": 0.01602403605408112,
        "f1": 0.007875880828235173,
        "hf_subset": "swa_Latn-heb_Hebr",
        "languages": [
          "swa-Latn",
          "heb-Hebr"
        ],
        "main_score": 0.007875880828235173,
        "precision": 0.00653817378328446,
        "recall": 0.01602403605408112
      },
      {
        "accuracy": 0.014021031547320982,
        "f1": 0.0042916862866419995,
        "hf_subset": "swa_Latn-hin_Deva",
        "languages": [
          "swa-Latn",
          "hin-Deva"
        ],
        "main_score": 0.0042916862866419995,
        "precision": 0.002925898831626008,
        "recall": 0.014021031547320982
      },
      {
        "accuracy": 0.05658487731597396,
        "f1": 0.037989423931672366,
        "hf_subset": "swa_Latn-hun_Latn",
        "languages": [
          "swa-Latn",
          "hun-Latn"
        ],
        "main_score": 0.037989423931672366,
        "precision": 0.03479238138628869,
        "recall": 0.05658487731597396
      },
      {
        "accuracy": 0.04807210816224337,
        "f1": 0.032761166763903136,
        "hf_subset": "swa_Latn-ibo_Latn",
        "languages": [
          "swa-Latn",
          "ibo-Latn"
        ],
        "main_score": 0.032761166763903136,
        "precision": 0.030722324459189355,
        "recall": 0.04807210816224337
      },
      {
        "accuracy": 0.1872809213820731,
        "f1": 0.15277756953561056,
        "hf_subset": "swa_Latn-ind_Latn",
        "languages": [
          "swa-Latn",
          "ind-Latn"
        ],
        "main_score": 0.15277756953561056,
        "precision": 0.14483319065076317,
        "recall": 0.1872809213820731
      },
      {
        "accuracy": 0.009514271407110666,
        "f1": 0.0021648507706705823,
        "hf_subset": "swa_Latn-jpn_Jpan",
        "languages": [
          "swa-Latn",
          "jpn-Jpan"
        ],
        "main_score": 0.0021648507706705823,
        "precision": 0.001523247974070581,
        "recall": 0.009514271407110666
      },
      {
        "accuracy": 0.022533800701051578,
        "f1": 0.010241303659701863,
        "hf_subset": "swa_Latn-kor_Hang",
        "languages": [
          "swa-Latn",
          "kor-Hang"
        ],
        "main_score": 0.010241303659701863,
        "precision": 0.008279509152669126,
        "recall": 0.022533800701051578
      },
      {
        "accuracy": 0.09364046069103656,
        "f1": 0.06302481259993833,
        "hf_subset": "swa_Latn-lit_Latn",
        "languages": [
          "swa-Latn",
          "lit-Latn"
        ],
        "main_score": 0.06302481259993833,
        "precision": 0.057105909646705436,
        "recall": 0.09364046069103656
      },
      {
        "accuracy": 0.12068102153229844,
        "f1": 0.08730277629447421,
        "hf_subset": "swa_Latn-nld_Latn",
        "languages": [
          "swa-Latn",
          "nld-Latn"
        ],
        "main_score": 0.08730277629447421,
        "precision": 0.08112952653390626,
        "recall": 0.12068102153229844
      },
      {
        "accuracy": 0.05358037055583375,
        "f1": 0.03439980882602149,
        "hf_subset": "swa_Latn-nso_Latn",
        "languages": [
          "swa-Latn",
          "nso-Latn"
        ],
        "main_score": 0.03439980882602149,
        "precision": 0.031498347438894965,
        "recall": 0.05358037055583375
      },
      {
        "accuracy": 0.020030045067601403,
        "f1": 0.011581471997321168,
        "hf_subset": "swa_Latn-orm_Ethi",
        "languages": [
          "swa-Latn",
          "orm-Ethi"
        ],
        "main_score": 0.011581471997321168,
        "precision": 0.010088671321975511,
        "recall": 0.020030045067601403
      },
      {
        "accuracy": 0.12068102153229844,
        "f1": 0.08671642000464179,
        "hf_subset": "swa_Latn-pol_Latn",
        "languages": [
          "swa-Latn",
          "pol-Latn"
        ],
        "main_score": 0.08671642000464179,
        "precision": 0.07962739126717253,
        "recall": 0.12068102153229844
      },
      {
        "accuracy": 0.1642463695543315,
        "f1": 0.12275506056077572,
        "hf_subset": "swa_Latn-por_Latn",
        "languages": [
          "swa-Latn",
          "por-Latn"
        ],
        "main_score": 0.12275506056077572,
        "precision": 0.11366929611073506,
        "recall": 0.1642463695543315
      },
      {
        "accuracy": 0.014521782674011016,
        "f1": 0.0058972053057301695,
        "hf_subset": "swa_Latn-rus_Cyrl",
        "languages": [
          "swa-Latn",
          "rus-Cyrl"
        ],
        "main_score": 0.0058972053057301695,
        "precision": 0.005015005927920578,
        "recall": 0.014521782674011016
      },
      {
        "accuracy": 0.04556835252879319,
        "f1": 0.032603738267966385,
        "hf_subset": "swa_Latn-som_Latn",
        "languages": [
          "swa-Latn",
          "som-Latn"
        ],
        "main_score": 0.032603738267966385,
        "precision": 0.030126218048656145,
        "recall": 0.04556835252879319
      },
      {
        "accuracy": 0.15823735603405106,
        "f1": 0.11735624709216973,
        "hf_subset": "swa_Latn-spa_Latn",
        "languages": [
          "swa-Latn",
          "spa-Latn"
        ],
        "main_score": 0.11735624709216973,
        "precision": 0.10851565333383244,
        "recall": 0.15823735603405106
      },
      {
        "accuracy": 0.03905858788182273,
        "f1": 0.023709062575291266,
        "hf_subset": "swa_Latn-ssw_Latn",
        "languages": [
          "swa-Latn",
          "ssw-Latn"
        ],
        "main_score": 0.023709062575291266,
        "precision": 0.021537626385611207,
        "recall": 0.03905858788182273
      },
      {
        "accuracy": 0.13219829744616926,
        "f1": 0.10176564118344084,
        "hf_subset": "swa_Latn-swe_Latn",
        "languages": [
          "swa-Latn",
          "swe-Latn"
        ],
        "main_score": 0.10176564118344084,
        "precision": 0.09481308022447836,
        "recall": 0.13219829744616926
      },
      {
        "accuracy": 0.018527791687531298,
        "f1": 0.007835169309931502,
        "hf_subset": "swa_Latn-tam_Taml",
        "languages": [
          "swa-Latn",
          "tam-Taml"
        ],
        "main_score": 0.007835169309931502,
        "precision": 0.006185807911371651,
        "recall": 0.018527791687531298
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.0057101385119390806,
        "hf_subset": "swa_Latn-tir_Ethi",
        "languages": [
          "swa-Latn",
          "tir-Ethi"
        ],
        "main_score": 0.0057101385119390806,
        "precision": 0.004813801871383426,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.05658487731597396,
        "f1": 0.03688368456057747,
        "hf_subset": "swa_Latn-tsn_Latn",
        "languages": [
          "swa-Latn",
          "tsn-Latn"
        ],
        "main_score": 0.03688368456057747,
        "precision": 0.03350866198260925,
        "recall": 0.05658487731597396
      },
      {
        "accuracy": 0.13119679519278918,
        "f1": 0.10013216834758287,
        "hf_subset": "swa_Latn-tur_Latn",
        "languages": [
          "swa-Latn",
          "tur-Latn"
        ],
        "main_score": 0.10013216834758287,
        "precision": 0.09293613145366386,
        "recall": 0.13119679519278918
      },
      {
        "accuracy": 0.03905858788182273,
        "f1": 0.022253064349416162,
        "hf_subset": "swa_Latn-vie_Latn",
        "languages": [
          "swa-Latn",
          "vie-Latn"
        ],
        "main_score": 0.022253064349416162,
        "precision": 0.019492387509271014,
        "recall": 0.03905858788182273
      },
      {
        "accuracy": 0.04256384576865298,
        "f1": 0.026092960872405608,
        "hf_subset": "swa_Latn-wol_Latn",
        "languages": [
          "swa-Latn",
          "wol-Latn"
        ],
        "main_score": 0.026092960872405608,
        "precision": 0.02326868450818136,
        "recall": 0.04256384576865298
      },
      {
        "accuracy": 0.031547320981472206,
        "f1": 0.02128430513847049,
        "hf_subset": "swa_Latn-xho_Latn",
        "languages": [
          "swa-Latn",
          "xho-Latn"
        ],
        "main_score": 0.02128430513847049,
        "precision": 0.01961289121249446,
        "recall": 0.031547320981472206
      },
      {
        "accuracy": 0.028542814221331998,
        "f1": 0.017333368663360722,
        "hf_subset": "swa_Latn-yor_Latn",
        "languages": [
          "swa-Latn",
          "yor-Latn"
        ],
        "main_score": 0.017333368663360722,
        "precision": 0.015573721038415362,
        "recall": 0.028542814221331998
      },
      {
        "accuracy": 0.013520280420630946,
        "f1": 0.007318352701786892,
        "hf_subset": "swa_Latn-zho_Hant",
        "languages": [
          "swa-Latn",
          "zho-Hant"
        ],
        "main_score": 0.007318352701786892,
        "precision": 0.0065613945260654975,
        "recall": 0.013520280420630946
      },
      {
        "accuracy": 0.05057586379569354,
        "f1": 0.034529892234252436,
        "hf_subset": "swa_Latn-zul_Latn",
        "languages": [
          "swa-Latn",
          "zul-Latn"
        ],
        "main_score": 0.034529892234252436,
        "precision": 0.03131048287114932,
        "recall": 0.05057586379569354
      },
      {
        "accuracy": 0.16875312969454181,
        "f1": 0.13090661349099975,
        "hf_subset": "swe_Latn-swa_Latn",
        "languages": [
          "swe-Latn",
          "swa-Latn"
        ],
        "main_score": 0.13090661349099975,
        "precision": 0.1216214755388198,
        "recall": 0.16875312969454181
      },
      {
        "accuracy": 0.007511266900350526,
        "f1": 0.005819229215847581,
        "hf_subset": "tam_Taml-swa_Latn",
        "languages": [
          "tam-Taml",
          "swa-Latn"
        ],
        "main_score": 0.005819229215847581,
        "precision": 0.005425293365267843,
        "recall": 0.007511266900350526
      },
      {
        "accuracy": 0.0030045067601402104,
        "f1": 0.002169921548990152,
        "hf_subset": "tir_Ethi-swa_Latn",
        "languages": [
          "tir-Ethi",
          "swa-Latn"
        ],
        "main_score": 0.002169921548990152,
        "precision": 0.0019529293940911366,
        "recall": 0.0030045067601402104
      },
      {
        "accuracy": 0.05257886830245368,
        "f1": 0.04361362982324409,
        "hf_subset": "tsn_Latn-swa_Latn",
        "languages": [
          "tsn-Latn",
          "swa-Latn"
        ],
        "main_score": 0.04361362982324409,
        "precision": 0.04166282875595276,
        "recall": 0.05257886830245368
      },
      {
        "accuracy": 0.12518778167250877,
        "f1": 0.09220859582858122,
        "hf_subset": "tur_Latn-swa_Latn",
        "languages": [
          "tur-Latn",
          "swa-Latn"
        ],
        "main_score": 0.09220859582858122,
        "precision": 0.08464267083216595,
        "recall": 0.12518778167250877
      },
      {
        "accuracy": 0.02303455182774161,
        "f1": 0.01959564026582269,
        "hf_subset": "vie_Latn-swa_Latn",
        "languages": [
          "vie-Latn",
          "swa-Latn"
        ],
        "main_score": 0.01959564026582269,
        "precision": 0.01888169300383764,
        "recall": 0.02303455182774161
      },
      {
        "accuracy": 0.031547320981472206,
        "f1": 0.025661649684916295,
        "hf_subset": "wol_Latn-swa_Latn",
        "languages": [
          "wol-Latn",
          "swa-Latn"
        ],
        "main_score": 0.025661649684916295,
        "precision": 0.024204884701021064,
        "recall": 0.031547320981472206
      },
      {
        "accuracy": 0.02754131196795193,
        "f1": 0.020546057748453722,
        "hf_subset": "xho_Latn-swa_Latn",
        "languages": [
          "xho-Latn",
          "swa-Latn"
        ],
        "main_score": 0.020546057748453722,
        "precision": 0.018910629551640575,
        "recall": 0.02754131196795193
      },
      {
        "accuracy": 0.02303455182774161,
        "f1": 0.019343944107817528,
        "hf_subset": "yor_Latn-swa_Latn",
        "languages": [
          "yor-Latn",
          "swa-Latn"
        ],
        "main_score": 0.019343944107817528,
        "precision": 0.018941950953450908,
        "recall": 0.02303455182774161
      },
      {
        "accuracy": 0.0035052578868302454,
        "f1": 0.0023406752477074778,
        "hf_subset": "zho_Hant-swa_Latn",
        "languages": [
          "zho-Hant",
          "swa-Latn"
        ],
        "main_score": 0.0023406752477074778,
        "precision": 0.0022553033950263026,
        "recall": 0.0035052578868302454
      },
      {
        "accuracy": 0.04456685027541312,
        "f1": 0.03468019398167,
        "hf_subset": "zul_Latn-swa_Latn",
        "languages": [
          "zul-Latn",
          "swa-Latn"
        ],
        "main_score": 0.03468019398167,
        "precision": 0.032664243341099485,
        "recall": 0.04456685027541312
      }
    ]
  },
  "task_name": "NTREXBitextMining"
}