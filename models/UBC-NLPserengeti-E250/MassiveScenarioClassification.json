{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 9.032449960708618,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "accuracy": 0.38241425689307335,
        "f1": 0.3668173377050275,
        "f1_weighted": 0.3879473737435445,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.38241425689307335,
        "scores_per_experiment": [
          {
            "accuracy": 0.3765971755211836,
            "f1": 0.36330300351807476,
            "f1_weighted": 0.38294914448222417
          },
          {
            "accuracy": 0.40685944855413586,
            "f1": 0.3904436746086886,
            "f1_weighted": 0.4136750735685879
          },
          {
            "accuracy": 0.37525218560860796,
            "f1": 0.36712619055117973,
            "f1_weighted": 0.37575441796446923
          },
          {
            "accuracy": 0.3937457969065232,
            "f1": 0.37948238749321844,
            "f1_weighted": 0.3905406966985598
          },
          {
            "accuracy": 0.3813046402151984,
            "f1": 0.36053620551783566,
            "f1_weighted": 0.3845356167933524
          },
          {
            "accuracy": 0.37020847343644925,
            "f1": 0.3593466769377114,
            "f1_weighted": 0.38035151915939586
          },
          {
            "accuracy": 0.38264963012777403,
            "f1": 0.36998634277029496,
            "f1_weighted": 0.3891160794816464
          },
          {
            "accuracy": 0.38197713517148624,
            "f1": 0.36225831833162064,
            "f1_weighted": 0.38945853362173777
          },
          {
            "accuracy": 0.38264963012777403,
            "f1": 0.3537305451562394,
            "f1_weighted": 0.3882629751568887
          },
          {
            "accuracy": 0.37289845326160054,
            "f1": 0.3619600321654117,
            "f1_weighted": 0.38482968050858285
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.3717166748647319,
        "f1": 0.3596090089741131,
        "f1_weighted": 0.37733539272302924,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.3717166748647319,
        "scores_per_experiment": [
          {
            "accuracy": 0.3635022134776193,
            "f1": 0.35367286422255195,
            "f1_weighted": 0.3731981217247056
          },
          {
            "accuracy": 0.40186915887850466,
            "f1": 0.3894316840894664,
            "f1_weighted": 0.40634288238070854
          },
          {
            "accuracy": 0.3777668470241023,
            "f1": 0.367430239437356,
            "f1_weighted": 0.3826493490189634
          },
          {
            "accuracy": 0.3561239547466798,
            "f1": 0.3477477660890338,
            "f1_weighted": 0.3567319626080742
          },
          {
            "accuracy": 0.3620265617314314,
            "f1": 0.3463403878705385,
            "f1_weighted": 0.36442407985203384
          },
          {
            "accuracy": 0.37383177570093457,
            "f1": 0.35923304630445446,
            "f1_weighted": 0.3806028395003597
          },
          {
            "accuracy": 0.3708804722085588,
            "f1": 0.3620508966809268,
            "f1_weighted": 0.3800424157060855
          },
          {
            "accuracy": 0.37383177570093457,
            "f1": 0.3525737144147198,
            "f1_weighted": 0.37948593340478626
          },
          {
            "accuracy": 0.367929168716183,
            "f1": 0.35446153436453476,
            "f1_weighted": 0.37248215421133113
          },
          {
            "accuracy": 0.3694048204623709,
            "f1": 0.36314795626754787,
            "f1_weighted": 0.3773941888232445
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}