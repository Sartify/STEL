{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 8.017606258392334,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "accuracy": 0.5056489576328176,
        "f1": 0.49817560939901273,
        "f1_weighted": 0.5073883048215285,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.5056489576328176,
        "scores_per_experiment": [
          {
            "accuracy": 0.5211835911230666,
            "f1": 0.5119342468010541,
            "f1_weighted": 0.5225138004934071
          },
          {
            "accuracy": 0.49159381304640215,
            "f1": 0.4769732308508673,
            "f1_weighted": 0.4926453505691305
          },
          {
            "accuracy": 0.5067249495628783,
            "f1": 0.5064093483655753,
            "f1_weighted": 0.5129359976924389
          },
          {
            "accuracy": 0.5057162071284466,
            "f1": 0.49739164190536056,
            "f1_weighted": 0.5054116560955008
          },
          {
            "accuracy": 0.5228648285137861,
            "f1": 0.5075258188878682,
            "f1_weighted": 0.5228486790055176
          },
          {
            "accuracy": 0.4899125756556826,
            "f1": 0.48850251552461466,
            "f1_weighted": 0.49172343337478397
          },
          {
            "accuracy": 0.5147948890383323,
            "f1": 0.5024566981339444,
            "f1_weighted": 0.515441706582427
          },
          {
            "accuracy": 0.49932750504371215,
            "f1": 0.4904185989493348,
            "f1_weighted": 0.5039296951448371
          },
          {
            "accuracy": 0.49159381304640215,
            "f1": 0.494163250006907,
            "f1_weighted": 0.4915494204666228
          },
          {
            "accuracy": 0.5127774041694687,
            "f1": 0.5059807445646008,
            "f1_weighted": 0.5148833087906202
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.5051647811116576,
        "f1": 0.5000375545649509,
        "f1_weighted": 0.5094582118588574,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.5051647811116576,
        "scores_per_experiment": [
          {
            "accuracy": 0.5090998524348254,
            "f1": 0.5042595598960214,
            "f1_weighted": 0.5131019579721525
          },
          {
            "accuracy": 0.4692572552877521,
            "f1": 0.46124864618410943,
            "f1_weighted": 0.46933596876333905
          },
          {
            "accuracy": 0.5130349237579931,
            "f1": 0.5096182197254266,
            "f1_weighted": 0.5243657777855664
          },
          {
            "accuracy": 0.5135268076733891,
            "f1": 0.5038178054397585,
            "f1_weighted": 0.5196136787594144
          },
          {
            "accuracy": 0.5258239055582883,
            "f1": 0.5136139614125413,
            "f1_weighted": 0.5281975614288313
          },
          {
            "accuracy": 0.4722085587801279,
            "f1": 0.47855335933887244,
            "f1_weighted": 0.47996976949905856
          },
          {
            "accuracy": 0.5243482538121004,
            "f1": 0.5126574018008456,
            "f1_weighted": 0.5271410760978757
          },
          {
            "accuracy": 0.5036891293654697,
            "f1": 0.4973579984613723,
            "f1_weighted": 0.511496032568486
          },
          {
            "accuracy": 0.5056566650270536,
            "f1": 0.5113510000300766,
            "f1_weighted": 0.5043826958256078
          },
          {
            "accuracy": 0.515002459419577,
            "f1": 0.5078975933604838,
            "f1_weighted": 0.5169775998882441
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}