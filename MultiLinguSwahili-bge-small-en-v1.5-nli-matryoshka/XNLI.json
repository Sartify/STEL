{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 1.3143954277038574,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.6065934065934065,
          "accuracy_threshold": 0.4300774931907654,
          "ap": 0.6230593262014602,
          "f1": 0.6746594005449591,
          "f1_threshold": 0.260009229183197,
          "precision": 0.5368603642671292,
          "recall": 0.907624633431085
        },
        "dot": {
          "accuracy": 0.6014652014652014,
          "accuracy_threshold": 0.2052285224199295,
          "ap": 0.6178584794301945,
          "f1": 0.6767337807606264,
          "f1_threshold": 0.13766932487487793,
          "precision": 0.5470162748643761,
          "recall": 0.8870967741935484
        },
        "euclidean": {
          "accuracy": 0.6124542124542125,
          "accuracy_threshold": 0.7497665882110596,
          "ap": 0.623684691976727,
          "f1": 0.6699555994079921,
          "f1_threshold": 0.9476191401481628,
          "precision": 0.5048327137546469,
          "recall": 0.9956011730205279
        },
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.6264585359089216,
        "manhattan": {
          "accuracy": 0.6021978021978022,
          "accuracy_threshold": 9.577261924743652,
          "ap": 0.6264585359089216,
          "f1": 0.6716343765524094,
          "f1_threshold": 11.85012435913086,
          "precision": 0.5078888054094666,
          "recall": 0.9912023460410557
        },
        "max": {
          "accuracy": 0.6124542124542125,
          "ap": 0.6264585359089216,
          "f1": 0.6767337807606264
        },
        "similarity": {
          "accuracy": 0.6065934065934065,
          "accuracy_threshold": 0.43007755279541016,
          "ap": 0.6230593262014602,
          "f1": 0.6746594005449591,
          "f1_threshold": 0.2600093185901642,
          "precision": 0.5368603642671292,
          "recall": 0.907624633431085
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.6263736263736264,
          "accuracy_threshold": 0.44315391778945923,
          "ap": 0.6446158018734394,
          "f1": 0.6829810901001113,
          "f1_threshold": 0.2811688184738159,
          "precision": 0.5501792114695341,
          "recall": 0.9002932551319648
        },
        "dot": {
          "accuracy": 0.6256410256410256,
          "accuracy_threshold": 0.20555312931537628,
          "ap": 0.6359668671515066,
          "f1": 0.6810154525386314,
          "f1_threshold": 0.1360113024711609,
          "precision": 0.5460176991150443,
          "recall": 0.9046920821114369
        },
        "euclidean": {
          "accuracy": 0.6249084249084249,
          "accuracy_threshold": 0.7641294002532959,
          "ap": 0.6468591607250591,
          "f1": 0.6869747899159664,
          "f1_threshold": 0.879572868347168,
          "precision": 0.5351882160392799,
          "recall": 0.9589442815249267
        },
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.6468591607250591,
        "manhattan": {
          "accuracy": 0.6227106227106227,
          "accuracy_threshold": 9.631715774536133,
          "ap": 0.6449803490243724,
          "f1": 0.6849604221635883,
          "f1_threshold": 11.116000175476074,
          "precision": 0.5350370981038747,
          "recall": 0.9516129032258065
        },
        "max": {
          "accuracy": 0.6263736263736264,
          "ap": 0.6468591607250591,
          "f1": 0.6869747899159664
        },
        "similarity": {
          "accuracy": 0.6263736263736264,
          "accuracy_threshold": 0.44315385818481445,
          "ap": 0.6446158018734394,
          "f1": 0.6829810901001113,
          "f1_threshold": 0.28116875886917114,
          "precision": 0.5501792114695341,
          "recall": 0.9002932551319648
        }
      }
    ]
  },
  "task_name": "XNLI"
}