{
  "dataset_revision": "09698e0180d87dc247ca447d3a1248b931ac0cdb",
  "evaluation_time": 1.3767080307006836,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "cosine": {
          "accuracy": 0.5567765567765568,
          "accuracy_threshold": 0.7911677360534668,
          "ap": 0.5700042548232325,
          "f1": 0.6666666666666665,
          "f1_threshold": 0.5707372426986694,
          "precision": 0.5014792899408284,
          "recall": 0.9941348973607038
        },
        "dot": {
          "accuracy": 0.5567765567765568,
          "accuracy_threshold": 0.7911677360534668,
          "ap": 0.5700042548232325,
          "f1": 0.6666666666666665,
          "f1_threshold": 0.5707372426986694,
          "precision": 0.5014792899408284,
          "recall": 0.9941348973607038
        },
        "euclidean": {
          "accuracy": 0.5567765567765568,
          "accuracy_threshold": 0.646269679069519,
          "ap": 0.5700042548232325,
          "f1": 0.6666666666666665,
          "f1_threshold": 0.9265657663345337,
          "precision": 0.5014792899408284,
          "recall": 0.9941348973607038
        },
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.5710042147405596,
        "manhattan": {
          "accuracy": 0.5567765567765568,
          "accuracy_threshold": 9.612199783325195,
          "ap": 0.5710042147405596,
          "f1": 0.6669965363681346,
          "f1_threshold": 14.135543823242188,
          "precision": 0.5033607169529499,
          "recall": 0.9882697947214076
        },
        "max": {
          "accuracy": 0.5567765567765568,
          "ap": 0.5710042147405596,
          "f1": 0.6669965363681346
        },
        "similarity": {
          "accuracy": 0.5567765567765568,
          "accuracy_threshold": 0.7911677360534668,
          "ap": 0.5700042548232325,
          "f1": 0.6666666666666665,
          "f1_threshold": 0.5707373023033142,
          "precision": 0.5014792899408284,
          "recall": 0.9941348973607038
        }
      }
    ],
    "validation": [
      {
        "cosine": {
          "accuracy": 0.6036630036630036,
          "accuracy_threshold": 0.7589627504348755,
          "ap": 0.595930480892997,
          "f1": 0.670020120724346,
          "f1_threshold": 0.6333744525909424,
          "precision": 0.5099540581929556,
          "recall": 0.9765395894428153
        },
        "dot": {
          "accuracy": 0.6036630036630036,
          "accuracy_threshold": 0.7589627504348755,
          "ap": 0.595930480892997,
          "f1": 0.670020120724346,
          "f1_threshold": 0.6333745121955872,
          "precision": 0.5099540581929556,
          "recall": 0.9765395894428153
        },
        "euclidean": {
          "accuracy": 0.6036630036630036,
          "accuracy_threshold": 0.6943158507347107,
          "ap": 0.595930480892997,
          "f1": 0.670020120724346,
          "f1_threshold": 0.8563000559806824,
          "precision": 0.5099540581929556,
          "recall": 0.9765395894428153
        },
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.595930480892997,
        "manhattan": {
          "accuracy": 0.6007326007326007,
          "accuracy_threshold": 11.104074478149414,
          "ap": 0.5933143767382916,
          "f1": 0.6686656671664168,
          "f1_threshold": 13.556123733520508,
          "precision": 0.5072024260803639,
          "recall": 0.9809384164222874
        },
        "max": {
          "accuracy": 0.6036630036630036,
          "ap": 0.595930480892997,
          "f1": 0.670020120724346
        },
        "similarity": {
          "accuracy": 0.6036630036630036,
          "accuracy_threshold": 0.7589627504348755,
          "ap": 0.595930480892997,
          "f1": 0.670020120724346,
          "f1_threshold": 0.6333745718002319,
          "precision": 0.5099540581929556,
          "recall": 0.9765395894428153
        }
      }
    ]
  },
  "task_name": "XNLI"
}