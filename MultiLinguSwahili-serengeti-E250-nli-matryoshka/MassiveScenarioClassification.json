{
  "dataset_revision": "fad2c6e8459f9e1c45d9315f4953d921437d70f8",
  "evaluation_time": 7.534860372543335,
  "kg_co2_emissions": null,
  "mteb_version": "1.12.68",
  "scores": {
    "test": [
      {
        "accuracy": 0.6120376597175521,
        "f1": 0.5983506192227048,
        "f1_weighted": 0.6131515452193921,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.6120376597175521,
        "scores_per_experiment": [
          {
            "accuracy": 0.6375252185608608,
            "f1": 0.6164737126702206,
            "f1_weighted": 0.6367729910561077
          },
          {
            "accuracy": 0.6099529253530599,
            "f1": 0.6018618646643802,
            "f1_weighted": 0.6130412084245116
          },
          {
            "accuracy": 0.6149966375252186,
            "f1": 0.6070678823771515,
            "f1_weighted": 0.6168616355317325
          },
          {
            "accuracy": 0.5965030262273033,
            "f1": 0.5852018516459646,
            "f1_weighted": 0.5965112619304465
          },
          {
            "accuracy": 0.5843981170141224,
            "f1": 0.5711388203974485,
            "f1_weighted": 0.5849341811650638
          },
          {
            "accuracy": 0.5887693342299932,
            "f1": 0.5615374553379285,
            "f1_weighted": 0.5863137488610569
          },
          {
            "accuracy": 0.6220578345662408,
            "f1": 0.6033889823440132,
            "f1_weighted": 0.6295358865886304
          },
          {
            "accuracy": 0.6049092131809012,
            "f1": 0.598383015115506,
            "f1_weighted": 0.6083250902087601
          },
          {
            "accuracy": 0.6314727639542703,
            "f1": 0.6191693189089795,
            "f1_weighted": 0.6311822430922016
          },
          {
            "accuracy": 0.6297915265635508,
            "f1": 0.6192832887654562,
            "f1_weighted": 0.6280372053354102
          }
        ]
      }
    ],
    "validation": [
      {
        "accuracy": 0.6121987211018199,
        "f1": 0.5974081008989192,
        "f1_weighted": 0.6128665272407073,
        "hf_subset": "sw",
        "languages": [
          "swa-Latn"
        ],
        "main_score": 0.6121987211018199,
        "scores_per_experiment": [
          {
            "accuracy": 0.6419085095917364,
            "f1": 0.6205859246387533,
            "f1_weighted": 0.6429055255187758
          },
          {
            "accuracy": 0.6138711264141663,
            "f1": 0.6082317814399611,
            "f1_weighted": 0.6159951996950745
          },
          {
            "accuracy": 0.6133792424987703,
            "f1": 0.605250778963263,
            "f1_weighted": 0.6193932640353658
          },
          {
            "accuracy": 0.5814067879980325,
            "f1": 0.5699661835311629,
            "f1_weighted": 0.581678055285289
          },
          {
            "accuracy": 0.5956714215445155,
            "f1": 0.5805405106800373,
            "f1_weighted": 0.5954118401834827
          },
          {
            "accuracy": 0.5809149040826365,
            "f1": 0.5552475331384865,
            "f1_weighted": 0.5748528434120166
          },
          {
            "accuracy": 0.6281357599606493,
            "f1": 0.6080112975314127,
            "f1_weighted": 0.6331168296635722
          },
          {
            "accuracy": 0.6015740285292671,
            "f1": 0.5896987575592892,
            "f1_weighted": 0.6034310639505305
          },
          {
            "accuracy": 0.6438760452533202,
            "f1": 0.6344160378949032,
            "f1_weighted": 0.6431986562965994
          },
          {
            "accuracy": 0.6212493851451057,
            "f1": 0.6021322036119225,
            "f1_weighted": 0.618681994366367
          }
        ]
      }
    ]
  },
  "task_name": "MassiveScenarioClassification"
}